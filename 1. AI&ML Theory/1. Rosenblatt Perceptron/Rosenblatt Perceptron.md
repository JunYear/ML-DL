# Rosenblatt Perceptron

## 생물학적 뉴런

- **세포체 (cell body)**, 여러 개의 **수상돌기 (dendrite)**, 하나의 **축색돌기 (axon)** 로 이루어짐.
- 뉴런 사이의 연결을 **시냅스 (synapse)** 라 부름.
- 뉴런은 추상돌기에서 자극을 받고, 자극이 충분할 경우 뉴런이 발동. (fire; 활성화 혹은 흥분)
- 축색돌기에서 자극을 출력하고, 이는 자극된 뉴런에 시냅스 연결을 갖는 다른 뉴런으로 전이.
- 시냅스 신호는 흥분적이거나 억제적임. (어떤 신호는 뉴런을 발동시키는 대신 발동을 막음)   

## 퍼셉트론 (Perceptron)
- 퍼셉트론은 **연산 유닛(computational unit)**, **다수의 입력** (**편향 입력(bias input)** 이 존재)으로 구성.
- 입력 각각은 결합된 입력 가중치와 단일 출력을 갖음.
- 입력과 출력은 각각 수상돌기와 축색돌기에 해당.
- 각 입력은 결합된 가중치를 가지며, 이는 **시냅스 가중치(Synapse weight)** 라 불러왔음.
- 이는 다른 의미로 한 뉴런에서 다른 뉴런으로의 연결이 얼마나 강한 지를 나타내기 때문에, 오늘날에는 **가중치 혹은 입력 가중치 (input weight)** 라고 부른다.
- 편향 입력은 오직 1로 출력은 -1 아니면 1이라는 두 값 중 하나만 취할 수 있다. (다른 형태의 인공 뉴런에서 실수값 범위로 완화할 수 있음)
- 각 입력값은 생물학 뉴런의 세포체에 해당하는 연산 유닛에 제시되기 전에 해당하는 가중치로 곱해짐.
- 연산 유닛은 입력의 가중된 합을 계산하며, 그런 다음 **활성화 함수 (activation function)** 라 부르는 y = f(z)를 적용함.
	- 이때, z는 가중된 입력의 합

> 퍼셉트론은 인공 뉴런의 한 형태이다. 이는 입력을 합하여 중간값 z를 계산하고, 활성화 함수에 공급한다. 퍼셉트론은 부호 함수를 활성화 함수로 사용하지만, 다른 인공 뉴런은 다른 함수를 사용한다.
>
> $\mathcal{y}$ $=$ $\mathcal{f}$ $($ $\mathcal{z}$ $)$ 
>
> $\mathcal{z}$ $=$ $\sum_{i=0}^n w_i x_i$ 
>
> $$f(z)=\begin{cases}-1, & \mbox{if }\mbox{z<0} \\1, & \mbox{if }\mbox{z>=0}\end{cases}$$
>
> $x_0$ $=$ $1$ (편향 항) 

- 편향 항 (bias term) $x_0$는 언제나 값 1로 할당됨.
- 해당 가중치 $w_0$는 다른 가중치들과 똑같이 다룸.


## 퍼셉트론 학습 알고리즘
- 퍼셉트론 학습 알고지즘은 **지도 학습 알고리즘(Supervised learning algorithm)**이라 부름
- 지도라는 개념은 훈련시키는 모델(model; 퍼셉트론에 해당)에 입력 데이터 및 원하는 출력 데이터 (ground truth; 정답)를 제시함을 뜻함
- 지도 학습의 반대는 비지도 학습(Unsupervised learning)으로, 이때 학습 알고지즘은 데이터에서 패턴을 스스로 찾을 책임을 짐

> 모델이란 용어는 네트워크와 같은 뜻을 갖는 경우가 종종 있다. 즉, 모델 훈련에 관해 이야기할 때 이는 하나 이상의 뉴런으로 구성된 네트워크를 위한 가중치를 찾는 것과 같다.

알고리즘의 동작
> 1. 가중치를 무작위로 초기화
> 2. 입력/출력 쌍 하나를 무작위로 선택
> 3. 값 $x_1$, ... ,$x_n$을 퍼셉트론에 제시하고 출력 y를 계산
> 4. 출력 $y$가 입력/출력 쌍의 정답과 다르면, 가중치를 다음과 같은 방법으로 조정
> 	a. $y < 0$ 이면, 각 $w_i$에서 $\eta x_i$를 더한다.
> 	b. $y > 0$ 이면, 각 $w_i$에서 $\eta x_i$를 뺀다.
> 5. 퍼셉트론이 모든 예시를 올바르게 예측할 때까지 2~4단계를 반복한다.

- 퍼셉트론은 예측할 수 있는 것에 있어서 특정 한계를 지니므로, 어떠한 입력/출력 쌍의 집합에서는 알고지즘이 수렴하지 않음
- 그러나 퍼셉트론이 입력/출력 쌍의 집합을 나타낼 수 있게 하는 가중치 집합을 찾을 수 있다면, 임의의 상수 $\eta$는 학습률이라 알려줘 있으며 이는 1로 둘 수 있지만, 알고리즘이 더 빨리 수렴하도록 다른 값으로 둘 수도 있음
- 학습률은 학습 알고리즘에 의해 조정되지는 않지만 여전히 조정 가능한 매개변수인 초매개변수(hyperparameter)의 예시 중 하나임